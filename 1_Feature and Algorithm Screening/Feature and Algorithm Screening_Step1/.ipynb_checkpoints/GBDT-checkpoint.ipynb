{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709092e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.feature_selection import RFE\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dd6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r\"C:\\Users\\HP\\Desktop\\Data.xlsx\",sheet_name='16+3',index_col=0,header=0)\n",
    "data1=data.iloc[0:16, :]\n",
    "minmax_scaler=preprocessing.MinMaxScaler() \n",
    "data2=minmax_scaler.fit_transform(data1)\n",
    "data3=pd.DataFrame(data2,columns=['O3_input','lg(O3)','H2O2_input','lg(H2O2)','H2O2/O3','lg(H2O2/O3)','pH','volumn',\n",
    "                                  'TOC','UV254','Φn1','Φn2','Φn3','Φn4','Φn5','FRI','FRI20','OU20','OU60','OU120',\n",
    "                                 'O3(1)20','O3(1)60','O3(1)120','O3(g)20','O3(g)60','O3(g)120','H2O2_20','H2O2_60',\n",
    "                                  'H2O2_120','FMax1','FMax2','FMax3','FMax4','cost'])\n",
    "data4=data3[['lg(O3)','lg(H2O2)','pH','TOC','UV254','Φn1','Φn2','Φn3','Φn4','Φn5','FRI','OU20','OU60','OU120',\n",
    "                                 'O3(1)20','O3(1)60','O3(1)120','O3(g)20','O3(g)60','O3(g)120','H2O2_20','H2O2_60',\n",
    "                                  'H2O2_120','FMax1','FMax2','FMax3','FMax4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f698fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | max_fe... | max_le... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6775   \u001b[39m | \u001b[39m0.08399  \u001b[39m | \u001b[39m3.881    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m5.93     \u001b[39m | \u001b[39m5.375    \u001b[39m | \u001b[39m55.25    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.6606   \u001b[39m | \u001b[39m0.03807  \u001b[39m | \u001b[39m2.382    \u001b[39m | \u001b[39m2.19     \u001b[39m | \u001b[39m9.005    \u001b[39m | \u001b[39m11.64    \u001b[39m | \u001b[39m345.8    \u001b[39m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comb \u001b[38;5;129;01min\u001b[39;00m combinations(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, X_original\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     71\u001b[0m     new_feature_subset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((initial_features,\n\u001b[0;32m     72\u001b[0m                                     X_original[:, comb[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     73\u001b[0m                                     X_original[:, comb[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m---> 76\u001b[0m     maxx2 \u001b[38;5;241m=\u001b[39m MODEL(new_feature_subset, y_original)\n\u001b[0;32m     79\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend((maxx2, new_feature_subset, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(comb)))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# All results are sorted in descending order according to maxx2 value.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 44\u001b[0m, in \u001b[0;36mMODEL\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     28\u001b[0m pbounds \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m500\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_leaf_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m     35\u001b[0m }\n\u001b[0;32m     37\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[0;32m     38\u001b[0m     f\u001b[38;5;241m=\u001b[39mblack_box_function,\n\u001b[0;32m     39\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[0;32m     40\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# verbose = 1 prints only当且仅当 a maximum is observed, verbose = 0 is silent\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mmaximize(\n\u001b[0;32m     45\u001b[0m     init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     46\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m maxx  \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax\n\u001b[0;32m     50\u001b[0m maxx2 \u001b[38;5;241m=\u001b[39mmaxx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\bayesian_optimization.py:374\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    372\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[0;32m    373\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobe(x_probe, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\bayesian_optimization.py:245\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mprobe(params)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\target_space.py:373\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[_hashable(x\u001b[38;5;241m.\u001b[39mravel())]\n\u001b[0;32m    372\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[1;32m--> 373\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mMODEL.<locals>.black_box_function\u001b[1;34m(learning_rate, n_estimators, min_samples_split, max_features, max_depth, max_leaf_nodes)\u001b[0m\n\u001b[0;32m     19\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X[train_index], X[test_index]\n\u001b[0;32m     20\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[1;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m y_real\u001b[38;5;241m.\u001b[39mappend(y_test[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    788\u001b[0m     X_train,\n\u001b[0;32m    789\u001b[0m     y_train,\n\u001b[0;32m    790\u001b[0m     raw_predictions,\n\u001b[0;32m    791\u001b[0m     sample_weight_train,\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    793\u001b[0m     X_val,\n\u001b[0;32m    794\u001b[0m     y_val,\n\u001b[0;32m    795\u001b[0m     sample_weight_val,\n\u001b[0;32m    796\u001b[0m     begin_at_stage,\n\u001b[0;32m    797\u001b[0m     monitor,\n\u001b[0;32m    798\u001b[0m )\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    884\u001b[0m     i,\n\u001b[0;32m    885\u001b[0m     X,\n\u001b[0;32m    886\u001b[0m     y,\n\u001b[0;32m    887\u001b[0m     raw_predictions,\n\u001b[0;32m    888\u001b[0m     sample_weight,\n\u001b[0;32m    889\u001b[0m     sample_mask,\n\u001b[0;32m    890\u001b[0m     random_state,\n\u001b[0;32m    891\u001b[0m     X_csc\u001b[38;5;241m=\u001b[39mX_csc,\n\u001b[0;32m    892\u001b[0m     X_csr\u001b[38;5;241m=\u001b[39mX_csr,\n\u001b[0;32m    893\u001b[0m )\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    490\u001b[0m     X, neg_g_view[:, k], sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1399\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \n\u001b[0;32m   1373\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1399\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1400\u001b[0m         X,\n\u001b[0;32m   1401\u001b[0m         y,\n\u001b[0;32m   1402\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1403\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1404\u001b[0m     )\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mE:\\ANACONDA\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def MODEL(X, y):\n",
    "\n",
    "    def black_box_function(learning_rate, n_estimators, min_samples_split, max_features, max_depth, max_leaf_nodes):\n",
    "        model = GradientBoostingRegressor(\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=min(max_features, 0.999),\n",
    "            max_depth=int(max_depth),\n",
    "            max_leaf_nodes=int(max_leaf_nodes),\n",
    "            random_state=2\n",
    "        )\n",
    "        loo = LeaveOneOut()\n",
    "        y_real = []\n",
    "        y_predicted = []\n",
    "\n",
    "        # Applying LOO-CV\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.fit(X_train, y_train.ravel())\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_real.append(y_test[0])\n",
    "            y_predicted.append(y_pred[0])\n",
    "        res = r2_score(y_real, y_predicted)\n",
    "        return res\n",
    "\n",
    "    pbounds = {\n",
    "        'learning_rate': (0.001, 0.2),\n",
    "        'n_estimators': (10, 500),\n",
    "        'min_samples_split': (2, 25),\n",
    "        'max_features': (1, 4),\n",
    "        'max_depth': (1, 5),\n",
    "        'max_leaf_nodes': (2, 15)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=black_box_function,\n",
    "        pbounds=pbounds,\n",
    "        verbose=2,  \n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=20,\n",
    "        n_iter=30\n",
    "    )\n",
    "\n",
    "    maxx  = optimizer.max\n",
    "    maxx2 =maxx['target']\n",
    "\n",
    "    return maxx2\n",
    "\n",
    "\n",
    "data7 = data4.values\n",
    "\n",
    "\n",
    "column_names = data4.columns\n",
    "\n",
    "X_original = data7[:, [i for i in range(data7.shape[1]) if i!= 3]]\n",
    "y_original = data7[:, 3]\n",
    "\n",
    "# The first three feature variables are selected as the initial feature subset.\n",
    "initial_features = X_original[:, :3]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# traverses the remaining variables, and selects two combinations with the first three features each time.\n",
    "for comb in combinations(range(3, X_original.shape[1]), 2):\n",
    "   \n",
    "    new_feature_subset = np.hstack((initial_features,\n",
    "                                    X_original[:, comb[0]].reshape(-1, 1),\n",
    "                                    X_original[:, comb[1]].reshape(-1, 1)))\n",
    "\n",
    "\n",
    "    maxx2 = MODEL(new_feature_subset, y_original)\n",
    "\n",
    "\n",
    "    all_results.append((maxx2, new_feature_subset, [0, 1, 2] + list(comb)))\n",
    "\n",
    "# All results are sorted in descending order according to maxx2 value.\n",
    "all_results.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Outputs the top 5 highest maxx2 values, corresponding feature subsets and feature indexes.\n",
    "for i in range(5):\n",
    "    print(f\"top {i + 1}maxx2 value:\", all_results[i][0])\n",
    "\n",
    "    data7_indexes = [index if index < 3 else index + 1 for index in all_results[i][2]]\n",
    "\n",
    "    print(f\"The corresponding feature subset:\", all_results[i][1])\n",
    "    print(f\"The corresponding index in data7:\", data7_indexes)\n",
    "    print(f\"The corresponding variable name:\", [column_names[index] for index in data7_indexes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
